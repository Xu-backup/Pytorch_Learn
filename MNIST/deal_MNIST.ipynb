{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载必要的库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms  #torchvison.datasets提供很多公开数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义超参数  认为定义的参数\n",
    "BATCH_SIZE = 128 #每批处理的数据\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #选择训练的设备是GPU还是CPU。\n",
    "EPOCHS = 10  #训练的轮次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 构建网络模型\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 10, 5) #1:灰度图片输入通道  10：输出通道(卷积核的个数)  5：kernel卷积核大小 （卷积层）\n",
    "        self.conv2 = nn.Conv2d(10, 20, 3)#10：输入通道 20：输出通道 3：kernel （卷积层）\n",
    "        self.fc1 = nn.Linear(20*10*10, 500)#20*10*10：输入通道 500：输出通道 全连接层\n",
    "        self.fc2 = nn.Linear(500, 10)#500：输入通道 10：输出通道 全连接层 (线性层) \n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2,ceil_mode=False)\n",
    "        self.relu = nn.ReLU() #inplace 是否覆盖input\n",
    "    \n",
    "    def forward(self,x):  #前向传播过程\n",
    "        input_size = x.size(0) #batch_size\n",
    "\n",
    "        x = self.conv1(x) #输入：batch_size*1*28*28 输出：batch_size*10*24*24 (24 = 28 - 5 + 1) （卷积层）\n",
    "        x = F.relu(x)  #激活函数，在所有隐藏层之间增加激活函数提高表达能力\n",
    "\n",
    "        #x = F.max_pool2d(x, 2, 2) #（池化层），对图片进行压缩，输入：bantch*10*24*24 输出：batch*10*12*12  取池化核中最大的值\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = self.conv2(x) #输入：batch*10*12*12 输出batch*20*10*10 （10 = 12 - 3 + 1）\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = x.view(input_size, -1) #拉平，-1自动计算维度，20*10*10=2000\n",
    "        #或者x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.fc1(x) #输入：batch*2000 输出：batch*500\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.fc2(x) #输入：batch*500 输出：batch*10\n",
    "\n",
    "        output = F.log_softmax(x,dim=1) #计算分类后，每个数字的概率值\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 定义训练方法\n",
    "def train_model(model, device, train_loader, optimizer, epoch):\n",
    "    #模型训练\n",
    "    model.train()\n",
    "    for batch_index, (data, target) in enumerate(train_loader):  #data为一个batch\n",
    "        #部署到device上\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        #初始化梯度\n",
    "        optimizer.zero_grad()\n",
    "        #训练后的结果\n",
    "        output = model(data)\n",
    "        #计算损失，多分类问题的损失（交叉熵）\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        #反向传播（将梯度返回model，优化器利用梯度优化参数）\n",
    "        loss.backward()\n",
    "        #参数优化(根据模型中的梯度，进行调优)\n",
    "        optimizer.step()\n",
    "        if batch_index % 3000 == 0:\n",
    "            print(f\"Train Epoch:{epoch}\\t Loss:{loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 定义测试方法\n",
    "def test_model(model, device, test_loader):\n",
    "    #模型验证\n",
    "    model.eval()\n",
    "    #正确率\n",
    "    correct = 0.0\n",
    "    #测试损失\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad(): #不计算梯度，不反向传播\n",
    "        for data, target in test_loader:\n",
    "            #部署到device\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            #测试数据\n",
    "            output = model(data)\n",
    "            #计算损失\n",
    "            test_loss += F.cross_entropy(output, target).item()\n",
    "            #获取预测值\n",
    "            pred = output.argmax(dim=1)\n",
    "            #累计正确的个数\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f\"Test Average Loss:{test_loss:.4f}, Accuracy:{correct/len(test_loader.dataset)*100:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 构建pipeline，对图像做处理，transform是pytorch为转换图片提供的工具\n",
    "pipeline = transforms.Compose([\n",
    "    transforms.ToTensor(),  #将图片转换为tensor\n",
    "    transforms.Normalize((0.1307,),(0.3081,)) #正则化降低模型复杂度，解决过拟合\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 下载，加载数据\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#下载数据集 \n",
    "#img, target = train_set[0] 获取数据集的内容和标签\n",
    "#train_set[0] 调用的是__getitem__()默认函数\n",
    "#test_set.classes[target] 获取标签名称\n",
    "#train_set和test_set 都是 Dataset类的实例（可以自行重写）  from torch.utils.data import Dataset \n",
    "train_set = datasets.MNIST(\"Data\", train=True, download=True, transform=pipeline)\n",
    "\n",
    "test_set = datasets.MNIST(\"Data\", train=False, download=True, transform=pipeline)\n",
    "\n",
    "'''img, target = train_set[0]\n",
    "print(img.shape)\n",
    "print(train_set.classes[target])'''\n",
    "\n",
    "#加载数据 \n",
    "#装载数据时确定了每个batch大小 batch_size变量\n",
    "#将batch_size个img和target进行打包返回\n",
    "#train_set和test_set 都是 Dataset类的实例（可以自行重写）\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True) \n",
    "\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'with open(\"./Data/MNIST/raw/t10k-images-idx3-ubyte\",\"rb\") as f:\\n    file = f.read()\\n\\nimage1 = [int(str(item).encode(\"ascii\"), 16) for item in file [16 : 16+784]]\\nimport cv2\\nimport numpy as np\\nimage1_np = np.array(image1, dtype=np.uint8).reshape(28,28,1)\\n\\nprint(image1_np.shape)\\ncv2.imwrite(\"digit.jpg\", image1_np)'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##查看 MNIST中文件的图片\n",
    "'''with open(\"./Data/MNIST/raw/t10k-images-idx3-ubyte\",\"rb\") as f:\n",
    "    file = f.read()\n",
    "\n",
    "image1 = [int(str(item).encode(\"ascii\"), 16) for item in file [16 : 16+784]]\n",
    "import cv2\n",
    "import numpy as np\n",
    "image1_np = np.array(image1, dtype=np.uint8).reshape(28,28,1)\n",
    "\n",
    "print(image1_np.shape)\n",
    "cv2.imwrite(\"digit.jpg\", image1_np)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## 定义优化器\n",
    "model = Net().to(DEVICE) ##模型实例化\n",
    "\n",
    "optimizer = optim.Adam(model.parameters()) ##调节模型参数的优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:1\t Loss:2.308753490447998\n",
      "Test Average Loss:0.0005, Accuracy:97.860\n",
      "Train Epoch:2\t Loss:0.04120075702667236\n",
      "Test Average Loss:0.0004, Accuracy:98.460\n",
      "Train Epoch:3\t Loss:0.02646901085972786\n",
      "Test Average Loss:0.0003, Accuracy:98.730\n",
      "Train Epoch:4\t Loss:0.023419199511408806\n",
      "Test Average Loss:0.0003, Accuracy:98.830\n",
      "Train Epoch:5\t Loss:0.0029025208204984665\n",
      "Test Average Loss:0.0003, Accuracy:98.870\n",
      "Train Epoch:6\t Loss:0.008113086223602295\n",
      "Test Average Loss:0.0002, Accuracy:99.110\n",
      "Train Epoch:7\t Loss:0.00480921333655715\n",
      "Test Average Loss:0.0002, Accuracy:99.000\n",
      "Train Epoch:8\t Loss:0.007507923990488052\n",
      "Test Average Loss:0.0003, Accuracy:98.800\n",
      "Train Epoch:9\t Loss:0.0014581014402210712\n",
      "Test Average Loss:0.0003, Accuracy:99.020\n",
      "Train Epoch:10\t Loss:0.01858961209654808\n",
      "Test Average Loss:0.0003, Accuracy:98.900\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS+1):\n",
    "    train_model(model, DEVICE, train_loader, optimizer, epoch)\n",
    "    test_model(model,DEVICE,test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov 24 2022, 14:38:14) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c920afd5e9cadd5260acd2f232a526811ef4cbf6cfe63966a0c8e5e8c6941a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
