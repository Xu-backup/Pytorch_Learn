{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的学习率： 0.1\n",
      "第1个epoch的学习率：0.100000\n",
      "第2个epoch的学习率：0.200000\n",
      "第3个epoch的学习率：0.300000\n",
      "第4个epoch的学习率：0.400000\n",
      "第5个epoch的学习率：0.500000\n",
      "第6个epoch的学习率：0.600000\n",
      "第7个epoch的学习率：0.700000\n",
      "第8个epoch的学习率：0.800000\n",
      "第9个epoch的学习率：0.900000\n",
      "第10个epoch的学习率：1.000000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "initial_lr = 0.1\n",
    "warmup_epochs = 10\n",
    " \n",
    "net_1=nn.Sequential(\n",
    "    nn.Linear(1,10)\n",
    ")\n",
    " \n",
    "optimizer_1 = torch.optim.Adam(\n",
    "    net_1.parameters(), \n",
    "    lr = initial_lr)\n",
    " \n",
    "##根据lambda 调整学习率\n",
    "#scheduler_1 = LambdaLR(optimizer_1, lr_lambda=lambda epoch: 1/(epoch+1) if epoch < warmup_epochs else 1 )\n",
    "scheduler_1 = torch.optim.lr_scheduler.LambdaLR(optimizer_1, lr_lambda=lambda epoch: (1+epoch*10/warmup_epochs) if epoch < warmup_epochs else 10)\n",
    " \n",
    "print(\"初始化的学习率：\", optimizer_1.defaults['lr'])\n",
    " \n",
    "for epoch in range(1, 11):\n",
    "    optimizer_1.zero_grad()\n",
    "    optimizer_1.step()\n",
    "    print(\"第%d个epoch的学习率：%f\" % (epoch, optimizer_1.param_groups[0]['lr']))\n",
    "    scheduler_1.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的学习率： 0.1\n",
      "第1个epoch的学习率：0.100000\n",
      "第2个epoch的学习率：0.090451\n",
      "第3个epoch的学习率：0.065451\n",
      "第4个epoch的学习率：0.034549\n",
      "第5个epoch的学习率：0.009549\n",
      "第6个epoch的学习率：0.000000\n",
      "第7个epoch的学习率：0.009549\n",
      "第8个epoch的学习率：0.034549\n",
      "第9个epoch的学习率：0.065451\n",
      "第10个epoch的学习率：0.090451\n"
     ]
    }
   ],
   "source": [
    "##在milestone进行调整学习率\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import *\n",
    "import matplotlib.pyplot as plt\n",
    "lst=[]\n",
    " \n",
    "initial_lr = 0.1\n",
    " \n",
    "net_1=nn.Sequential(\n",
    "    nn.Linear(1,10)\n",
    ")\n",
    " \n",
    "optimizer_1 = torch.optim.Adam(net_1.parameters(), lr = initial_lr)\n",
    "##根据节点调整学习率 \n",
    "scheduler_1 = MultiStepLR(optimizer_1, milestones=[3,9], gamma=0.1)\n",
    "##根据线性衰减\n",
    "T = 5\n",
    "scheduler_1 = LambdaLR(optimizer_1, lr_lambda=lambda step: 1 - step / T if step<0.9*T else 0.1  )\n",
    "\n",
    "##根据余弦衰减\n",
    "scheduler_1 = CosineAnnealingLR(optimizer_1, T_max=T, eta_min=0)\n",
    " \n",
    "print(\"初始化的学习率：\", optimizer_1.defaults['lr'])\n",
    " \n",
    "for epoch in range(1, 11):\n",
    "    # train\n",
    " \n",
    "    optimizer_1.zero_grad()\n",
    "    optimizer_1.step()\n",
    "    print(\"第%d个epoch的学习率：%f\" % (epoch, optimizer_1.param_groups[0]['lr']))\n",
    "    lst.append( optimizer_1.param_groups[0]['lr'])\n",
    "    scheduler_1.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
